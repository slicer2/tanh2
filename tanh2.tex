\title{MMSE Decoding of Linear Block Code}
\author{
  Dan Zhang
}
\date{\today}

\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}

\newcommand{\vect}[1]{\mathbf{#1}}
\begin{document}
\maketitle

\section{Introduction}
Conventionally a binary linear block code is specified by a $k\times n$ generator matrix $G$ in the binary field $\{0,1\}$, such that message $\vect{c}=c_1\cdots c_k$ is encoded into a code word $\vect{b} = b_1\cdots b_n$ through
\begin{align}
\vect{b} = \vect{c}G
\end{align}
Such a code word $\vect{b}$ will satisfy the parity check condition
\begin{align}
\vect{b}H^T=0,
\end{align}
where $H$ is the associated $(n-k)\times n$ parity check matrix. When $G$ can be paritioned as $G=\begin{bmatrix}I &P\end{bmatrix}$, the code is systematic and $H = \begin{bmatrix}P^T & I\end{bmatrix}$. In what follows we assume the code is systematic only for simplicity. Conclusions are applicable to non-systematic codes as well.

The rows of $H$ span a vector space that is denoted as $\mathrm{span}(H)$. Given any row vector $\vect{h}$ from $\mathrm{span}(H)$, its $i$-th component (from the binary field $\{0,1\}$) is denoted as $h_i$. We use $\vect{e}_i$ to denote the vector whose $i$-th component is $1$ and all the other components are $0$. Two vectors can be added together componentwise. Such addition is denoted with $\oplus$. A vector can be added to a vector space. The result is a set obtained from adding that vector to every vector in the space. For example, $\mathrm{span}(H)\oplus \vect{e}_i$ stands for a set obtained by adding $\vect{e}_i$ to every vector in $\mathrm{span}(H)$.

Further, we assume antipodal signaling is employed such that the actually transmitted bits are represented by $\pm 1$, with the mapping rule: $0\rightarrow 1$ and $1\rightarrow -1$. Under antipodal signaling, bit multiplication is equivalent to binary field addition -- a useful property for the development in this article.

Given $\vect{b}$, a memory-less channel (Gaussian, BSC, BEC, for example), and the observations $\vect{y}=y_1y_2\cdots y_n$, MAP decoding of binary linear block code is based on the LLR
\begin{align}
\Lambda_i = \log \frac{P(\vect{y}|b_i=1)}{P(\vect{y}|b_i=-1)}.
\end{align}
$b_i$ is decoded as $1$ if $\Lambda_i>0$; or $-1$ otherwise. Another conceivable decoding criteria is by computing
\begin{align}
E[b_i|\vect{y}],  \label{MMSEDecoding}
\end{align}
$b_i$ is decoded as $1$ if $E[b_i|\vect{y}]>0$; or $-1$ otherwise. Because
\begin{align}
\hat{b}_i = E[b_i|\vect{y}]
\end{align}
minimizes
\begin{align*}
E[|\hat{b}_i-b_i|^2]
\end{align*}
we can call this type of decoding the MMSE decoding. It is shown below that MMSE decoding is equivalent to MAP decoding though MMSE decoding enjoys a closed form expression: If we define
\begin{align}
E_i = E[b_i|y_i]
\end{align}
we can write out (\ref{MMSEDecoding}) in terms of $E_i$. We will give a few examples. The expression also leads to a different form of the famous sum-product algorithm for iterative decoding.

\section{The Closed Form}
Given the observation $y$ as the result of feeding a bit $b$ through the channel, we have single bit posterior expectation
\begin{align}
E[b|y] = \frac{P(y|b=1)-P(y|b=-1)}{P(y|b=1)+P(y|b=-1)} \label{single_observation_mmse}
\end{align}
Given a code word $\vect{b} = b_1b_2\cdots b_n$, its corresponding observation $\vect{y}$, and the bitwise posterior expecation
\begin{align}
E_i = E[b_i|y_i],
\end{align}
we can show
\begin{align}
E[b_i|\vect{y}] = \frac{\sum_{\vect{h}\in \mathrm{span} (H)\oplus \vect{e}_i}\prod_{i\atop h_i=1}E_i}{\sum_{\vect{h}\in \mathrm{span} (H)}\prod_{i\atop h_i=1}E_i}.
\end{align}

\begin{proof}
Similar to (\ref{single_observation_mmse})
\begin{align}
E[b_i|\vect{y}] &= \frac{P(\vect{y}|b_i=1)-P(\vect{y}|b_i=-1)}{P(\vect{y}|b_i=1)+P(\vect{y}|b_i=-1)}\\
&=\frac{\sum_{b_1}P(y_1|b_1)\cdots\sum_{b_i}b_iP(y_i|b_i)\cdots\sum_{b_k}P(y_k|b_k)P(y_{k+1}|b_{k+1})\cdots P(y_n|b_n)}{\sum_{b_1}P(y_1|b_1)\cdots\sum_{b_i}P(y_i|b_i)\cdots\sum_{b_k}P(y_k|b_k)P(y_{k+1}|b_{k+1})\cdots P(y_n|b_n)}. \label{mmse_conditional_probability}
\end{align}
Summatoin is over $b_1, \ldots, b_k$ as the code is systematic. Let
\begin{align}
N_j = P(y_j|b_j=1)-P(y_j|b_j=1), D_j = P(y_j|b_j=1)+P(y_j|b_j=-1)
\end{align}
then
\begin{align}
P(y_j|b_j=1) = (D_j+N_j)/2, P(y_j|b_j=-1) = (D_j-N_j)/2
\end{align}
or more compactly
\begin{align}
P(y_j|b_j) = (D_j+b_jN_j)/2 \label{conditional_probability_nd}
\end{align}
Plug (\ref{conditional_probability_nd}) into the denominator of (\ref{mmse_conditional_probability}), we have
\begin{align}
&\text{Denominator}\\
=&\sum_{b_1}\frac{D_1+b_1N_1}{2}\cdots\sum_{b_i}\frac{D_i+b_iN_i}{2}\cdots\sum_{b_k}\frac{D_k+b_kN_k}{2}\frac{D_{k+1}+b_{k+1}N_{k+1}}{2}\cdots\frac{D_n+b_nN_n}{2}
\end{align}
Expanding the expression we get a posynomial in $D_j$, $b_j$ and $N_j$. Therefore we may write it as
\begin{align}
&\text{Denominator}\\
=&2^{-n}\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in \{0,1\}^n}\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell
\end{align}
Split $\{0,1\}^n=H\cup H^c$ and evaluate them separately
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in H}\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h(\ell) = 1}b_\ell N_\ell\\
=&2^k\sum_{\vect{h}\in H}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
The last step is true because
\begin{align}
\prod_{\ell\atop h(\ell) = 1}b_\ell =1, \forall h\in H
\end{align}
But $\forall h \in H^c$
\begin{align}
\prod_{\ell\atop h(\ell) = 1}b_\ell =b_{\ell_1}\cdots b_{\ell_m}\neq 1
\end{align}
for some $m>0$ and $1\le \ell_1 < \cdots < \ell_m\le k$. Therefore
\begin{align}
\sum_{b_1}\cdots\sum_{b_k}b_{\ell_1}\cdots b_{\ell_m} = 2^{k-m}\left(\sum_{b_{\ell_1}}b_{\ell_1}\right)\cdots\left(\sum_{b_{\ell_m}}b_{\ell_m}\right)=0
\end{align}
Consequently
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in H^c}\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell\\
=&\sum_{\vect{h}\in H^c}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)\left(\sum_{b_1}\cdots\sum_{b_k}\prod_{\ell\atop h_\ell=1}b_\ell\right)\\
=&0
\end{align}
Therefore
\begin{align}
\text{Denominator}=2^{k-n}\sum_{\vect{h}\in H}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
The numerator can be written as
\begin{align}
&\text{Numerator}\\
=&\sum_{b_1}\frac{D_1+b_1N_1}{2}\cdots\sum_{b_i}b_i\frac{D_i+b_iN_i}{2}\cdots\sum_{b_k}\frac{D_k+b_kN_k}{2}\frac{D_{k+1}+b_{k+1}N_{k+1}}{2}\cdots\frac{D_n+b_nN_n}{2}
\end{align}
Expanding it, we get a posynomial
\begin{align}
&\text{Numerator}\\
=&2^{-n}\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in \{0,1\}^n}b_i\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell
\end{align}
From this expression, we can find, in a similar way
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in H\oplus \vect{e}_i}b_i\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell\\
=&2^{k-n}\sum_{\vect{h}\in H\oplus \vect{e}_i}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
and
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\not\in H\oplus\vect{e}_i}b_i\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell\\
=&0
\end{align}
So
\begin{align}
\text{Numerator}=2^{k-n}\sum_{\vect{h}\in H\oplus\vect{e}_i}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
We can finally write
\begin{align}
E[b_i|\vect{y}] &= \frac{\sum_{\vect{h}\in H\oplus\vect{e}_i}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)}{\sum_{\vect{h}\in H}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)}
\end{align}
Dividing numerator and denominator by $\prod_{j=1}^nD_j$, we get
\begin{align}
E[b_i|\vect{y}] &=\frac{\sum_{\vect{h}\in H\oplus\vect{e}_i}\prod_{\ell\atop h_\ell = 1} \frac{N_\ell}{D_\ell}}{\sum_{\vect{h}\in H}\prod_{\ell\atop h_\ell = 1} \frac{N_\ell}{D_\ell}}\\
&=\frac{\sum_{\vect{h}\in H\oplus\vect{e}_i}\prod_{\ell\atop h_\ell = 1} E[y_\ell|b_\ell]}{\sum_{\vect{h}\in H}\prod_{\ell\atop h_\ell = 1} E[y_\ell|b_\ell]}
\end{align}
\end{proof}
\section{Specialization}
\subsection{BEC}
\subsection{BSC}
\subsection{AWGN}
\section{Belief Propogation}
Belief propogation using MMSE representation has the following iterative form:

From variable node $a$ to factor node $\alpha$:
\begin{align}
m_{a\rightarrow\alpha}^{(n)}=\tanh\left(\tanh^{-1}(E_a)+\sum_{\beta\in F(a)\atop \beta \neq \alpha}\tanh^{-1}(M_{\beta\rightarrow  a}^{(n-1)})\right), \label{e:n2f}
\end{align}
and from factor node $\alpha$ to variable node $a$:
\begin{align}
M_{\alpha\rightarrow a}^{(n)} = \prod_{b\in N(\alpha)\atop b \neq a}m_{b\rightarrow \alpha}^{(n)}. \label{e:f2n}
\end{align}
with
\begin{align}
M_{\beta\rightarrow a}^{(0)} = 0.
\end{align}
After $n$ iterations ($n$ being sufficiently large), MMSE of node $a$ is recovered as
\begin{align}
E[a|\vect{y}] = \tanh\left(\tanh^{-1}(E_a)+\sum_{\beta\in F(a)}\tanh^{-1}(M_{\beta\rightarrow  a}^{(n)})\right),
\end{align}
The BP algorithm does not guarantee to converge to the correct value of $E[a|\vect{y}]$ for cyclic factor graphs. We show the proof here that convergence is guaranteed for acyclic factor graphs by induction on $p$, the number of parity checks. If $p=1$, as shown in Fig. \ref{f:p=1}, convergence is achieved after one iteration:
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{p1}
\caption{Factor graph when $p=1$.}
\label{f:p=1}
\end{figure}
Assume it is correct up to $p-1$, now add one more factor node $\gamma$. Assume $\gamma$ connects to variable node $x, x_1, x_2, \ldots, x_g$ and $x$ is the only common variable node connected to the other $p-1$ factor nodes -- a necessay consequence if the factor graph derived from the $p$ parity checks has no cycle. Follow iterations (\ref{e:n2f}) and (\ref{e:f2n}), we have
\begin{align*}
M_{\gamma\rightarrow x}^{(1)} = E_{x_1}E_{x_2}\cdots E_{x_g}.
\end{align*}
In the iteration 2, the message from $x$ towards factor nodes other than $\gamma$ will take the form
\begin{align*}
&\tanh\left(\tanh^{-1}(E_x) + \tanh^{-1}(M_{\gamma\rightarrow x})+\cdots \right)\\
=&\tanh\left(\tanh^{-1}\left(\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}\right)+\cdots\right)\\
=&\tanh\left(\tanh^{-1}\left(\frac{E_x + E_{x_1}E_{x_2}\cdots E_{x_g}}{1+E_xE_{x_1}E_{x_2}\cdots E_{x_g}}\right)+\cdots\right)
\end{align*}
This is equivalent to the graph with $p-1$ factor nodes, except all occurences of $E_x$ in the messages are to be replaced by
\begin{align*}
\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}
\end{align*}
By induction, with $p-1$ factor nodes, BP computes the correct MMSE. Suppose it is given by
\begin{align*}
\frac{E_xP_1+P_2}{E_xQ_1+Q_2}
\end{align*}
where $P_1, P_2, Q_1, Q_2$ are posynomials not involving $E_x$. With the additional factor node, BP will actually compute
\begin{align*}
&\frac{\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}P_1+P_2}{\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}Q_1+Q_2}\\
=&\frac{E_xP_1+P_2+M_{\gamma\rightarrow x}P_1 + E_xM_{\gamma\rightarrow x}P_2}{E_xQ_1+Q_2+M_{\gamma\rightarrow x}Q_1 + E_xM_{\gamma\rightarrow x}Q_2}\\
=&\frac{E_xP_1+P_2+E_{x_1}E_{x_2}\cdots E_{x_g}P_1 + E_xE_{x_1}E_{x_2}\cdots E_{x_g}P_2}{E_xQ_1+Q_2+E_{x_1}E_{x_2}\cdots E_{x_g}Q_1 + E_xE_{x_1}E_{x_2}\cdots E_{x_g}Q_2}
\end{align*}
This is the correct MMSE with $p$ factors. For MMSE of node $x_1$, $x_1$ will receive a message of the form
\begin{align*}
m_{x\rightarrow\gamma}E_{x_2}\cdots E_{x_g}
\end{align*}
where $m_{x\rightarrow\gamma}$ is the MMSE of $x$ given $p-1$ parity checks. In other words
\begin{align}
m_{x\rightarrow\gamma} = \frac{E_xP_2+P_1}{E_xP_1+P_2}
\end{align}
where 
\begin{align*}
E_xP_1+P_2
\end{align*}
is the characteristic polynomial of $H_{p-1}$ the vector space spanned by the first $p-1$ parity checks. Node $x_1$ will then form the MMSE of itself
\begin{align}
&\tanh\left(\tanh^{-1}(E_{x_1})+\tanh^{-1}\left(m_{x\rightarrow\gamma}E_{x_2}\cdots E_{x_g}\right)\right)\\
=&\frac{E_{x_1}+\frac{E_xP_2+P_1}{E_xP_1+P_2}E_{x_2}\cdots E_{x_g}}{1+E_{x_1}E_{x_2}\cdots E_{x_g}\frac{E_xP_2+P_1}{E_xP_1+P_2}}\\
=&\frac{E_{x_1}(E_xP_1+P_2)+(E_xP_2+P_1)E_{x_2}\cdots E_{x_g}}{E_xP_1+P_2+E_{x_1}\cdots E_{x_g}(E_xP_2+P_1)}
\end{align}
One can verify the denominator is indeed the characteristic polynomial of the $p$ parity checks and the fraction forms the MMSE of $x_1$.
\end{document}

