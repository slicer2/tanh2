\title{MMSE Decoding of Linear Block Code}
\author{
  Dan Zhang
}
\date{\today}

\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}

\newcommand{\vect}[1]{\mathbf{#1}}
\begin{document}
\maketitle

\section{Introduction}
Conventionally a binary linear block code is specified by a $k\times n$ generator matrix $G$ in the binary field $\{0,1\}$, such that message $\vect{c}=c_1\cdots c_k$ is encoded into a code word $\vect{b} = b_1\cdots b_n$ through
\begin{align}
\vect{b} = \vect{c}G
\end{align}
Such a code word $\vect{b}$ will satisfy the parity check condition
\begin{align}
\vect{b}H^T=0,
\end{align}
where $H$ is the associated $(n-k)\times n$ parity check matrix. When $G$ can be paritioned as $G=\begin{bmatrix}I &P\end{bmatrix}$, the code is systematic and $H = \begin{bmatrix}P^T & I\end{bmatrix}$. In what follows we assume the code is systematic only for simplicity. Conclusions are applicable to non-systematic codes as well.

The rows of $H$ span a vector space that is denoted as $\mathrm{span}(H)$. Given any row vector $\vect{h}$ from $\mathrm{span}(H)$, its $i$-th component (from the binary field $\{0,1\}$) is denoted as $h_i$. We use $\vect{e}_i$ to denote the vector whose $i$-th component is $1$ and all the other components are $0$. Two vectors can be added together componentwise. Such addition is denoted with $\oplus$. A vector can be added to a vector space. The result is a set obtained from adding that vector to every vector in the space. For example, $\mathrm{span}(H)\oplus \vect{e}_i$ stands for a set obtained by adding $\vect{e}_i$ to every vector in $\mathrm{span}(H)$.

Further, we assume antipodal signaling is employed such that the actually transmitted bits are represented by $\pm 1$, with the mapping rule: $0\rightarrow 1$ and $1\rightarrow -1$. Under antipodal signaling, bit multiplication is equivalent to binary field addition -- a useful property for the development in this article.

Given $\vect{b}$, a memory-less channel (Gaussian, BSC, BEC, for example), and the observations $\vect{y}=y_1y_2\cdots y_n$, MAP decoding of binary linear block code is based on the LLR
\begin{align}
\Lambda_i = \log \frac{P(\vect{y}|b_i=1)}{P(\vect{y}|b_i=-1)}.
\end{align}
$b_i$ is decoded as $1$ if $\Lambda_i>0$; or $-1$ otherwise. Another conceivable decoding criteria is by computing
\begin{align}
E[b_i|\vect{y}],  \label{MMSEDecoding}
\end{align}
$b_i$ is decoded as $1$ if $E[b_i|\vect{y}]>0$; or $-1$ otherwise. Because
\begin{align}
\hat{b}_i = E[b_i|\vect{y}]
\end{align}
minimizes
\begin{align*}
E[|\hat{b}_i-b_i|^2]
\end{align*}
we can call this type of decoding the MMSE decoding. It is shown below that MMSE decoding is equivalent to MAP decoding though MMSE decoding enjoys a closed form expression: If we define
\begin{align}
E_i = E[b_i|y_i]
\end{align}
we can write out (\ref{MMSEDecoding}) in terms of $E_i$. We will give a few examples. The expression also leads to a different form of the famous sum-product algorithm for iterative decoding.

\section{The Closed Form}
Given the observation $y$ as the result of feeding a bit $b$ through the channel, we have single bit posterior expectation
\begin{align}
E[b|y] = \frac{P(y|b=1)-P(y|b=-1)}{P(y|b=1)+P(y|b=-1)} \label{e:single_observation_mmse}
\end{align}
Given a code word $\vect{b} = b_1b_2\cdots b_n$, its corresponding observation $\vect{y}$, and the bitwise posterior expecation
\begin{align}
E_i = E[b_i|y_i],
\end{align}
we can show
\begin{align}
E[b_i|\vect{y}] = \frac{\sum_{\vect{h}\in \mathrm{span} (H)\oplus \vect{e}_i}\prod_{i\atop h_i=1}E_i}{\sum_{\vect{h}\in \mathrm{span} (H)}\prod_{i\atop h_i=1}E_i}. \label{e:closedform}
\end{align}

\begin{proof}
Similar to (\ref{e:single_observation_mmse})
\begin{align}
E[b_i|\vect{y}] &= \frac{P(\vect{y}|b_i=1)-P(\vect{y}|b_i=-1)}{P(\vect{y}|b_i=1)+P(\vect{y}|b_i=-1)}\\
&=\frac{\sum_{b_1}P(y_1|b_1)\cdots\sum_{b_i}b_iP(y_i|b_i)\cdots\sum_{b_k}P(y_k|b_k)P(y_{k+1}|b_{k+1})\cdots P(y_n|b_n)}{\sum_{b_1}P(y_1|b_1)\cdots\sum_{b_i}P(y_i|b_i)\cdots\sum_{b_k}P(y_k|b_k)P(y_{k+1}|b_{k+1})\cdots P(y_n|b_n)}. \label{mmse_conditional_probability}
\end{align}
Summatoin is over $b_1, \ldots, b_k$ as the code is systematic. Let
\begin{align}
N_j = P(y_j|b_j=1)-P(y_j|b_j=1), D_j = P(y_j|b_j=1)+P(y_j|b_j=-1)
\end{align}
then
\begin{align}
P(y_j|b_j=1) = (D_j+N_j)/2, P(y_j|b_j=-1) = (D_j-N_j)/2
\end{align}
or more compactly
\begin{align}
P(y_j|b_j) = (D_j+b_jN_j)/2 \label{conditional_probability_nd}
\end{align}
Plug (\ref{conditional_probability_nd}) into the denominator of (\ref{mmse_conditional_probability}), we have
\begin{align}
&\text{Denominator}\\
=&\sum_{b_1}\frac{D_1+b_1N_1}{2}\cdots\sum_{b_i}\frac{D_i+b_iN_i}{2}\cdots\sum_{b_k}\frac{D_k+b_kN_k}{2}\frac{D_{k+1}+b_{k+1}N_{k+1}}{2}\cdots\frac{D_n+b_nN_n}{2}
\end{align}
Expanding the expression we get a posynomial in $D_j$, $b_j$ and $N_j$. Therefore we may write it as
\begin{align}
&\text{Denominator}\\
=&2^{-n}\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in \{0,1\}^n}\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell
\end{align}
Split $\{0,1\}^n=H\cup H^c$ and evaluate them separately
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in H}\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h(\ell) = 1}b_\ell N_\ell\\
=&2^k\sum_{\vect{h}\in H}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
The last step is true because
\begin{align}
\prod_{\ell\atop h(\ell) = 1}b_\ell =1, \forall h\in H
\end{align}
But $\forall h \in H^c$
\begin{align}
\prod_{\ell\atop h(\ell) = 1}b_\ell =b_{\ell_1}\cdots b_{\ell_m}\neq 1
\end{align}
for some $m>0$ and $1\le \ell_1 < \cdots < \ell_m\le k$. Therefore
\begin{align}
\sum_{b_1}\cdots\sum_{b_k}b_{\ell_1}\cdots b_{\ell_m} = 2^{k-m}\left(\sum_{b_{\ell_1}}b_{\ell_1}\right)\cdots\left(\sum_{b_{\ell_m}}b_{\ell_m}\right)=0
\end{align}
Consequently
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in H^c}\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell\\
=&\sum_{\vect{h}\in H^c}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)\left(\sum_{b_1}\cdots\sum_{b_k}\prod_{\ell\atop h_\ell=1}b_\ell\right)\\
=&0
\end{align}
Therefore
\begin{align}
\text{Denominator}=2^{k-n}\sum_{\vect{h}\in H}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
The numerator can be written as
\begin{align}
&\text{Numerator}\\
=&\sum_{b_1}\frac{D_1+b_1N_1}{2}\cdots\sum_{b_i}b_i\frac{D_i+b_iN_i}{2}\cdots\sum_{b_k}\frac{D_k+b_kN_k}{2}\frac{D_{k+1}+b_{k+1}N_{k+1}}{2}\cdots\frac{D_n+b_nN_n}{2}
\end{align}
Expanding it, we get a posynomial
\begin{align}
&\text{Numerator}\\
=&2^{-n}\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in \{0,1\}^n}b_i\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell
\end{align}
From this expression, we can find, in a similar way
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\in H\oplus \vect{e}_i}b_i\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell\\
=&2^{k-n}\sum_{\vect{h}\in H\oplus \vect{e}_i}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
and
\begin{align}
&\sum_{b_1}\cdots\sum_{b_k}\sum_{\vect{h}\not\in H\oplus\vect{e}_i}b_i\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1}b_\ell N_\ell\\
=&0
\end{align}
So
\begin{align}
\text{Numerator}=2^{k-n}\sum_{\vect{h}\in H\oplus\vect{e}_i}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)
\end{align}
We can finally write
\begin{align}
E[b_i|\vect{y}] &= \frac{\sum_{\vect{h}\in H\oplus\vect{e}_i}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)}{\sum_{\vect{h}\in H}\left(\prod_{j\atop h_j = 0}D_j\prod_{\ell\atop h_\ell = 1} N_\ell\right)}
\end{align}
Dividing numerator and denominator by $\prod_{j=1}^nD_j$, we get
\begin{align}
E[b_i|\vect{y}] &=\frac{\sum_{\vect{h}\in H\oplus\vect{e}_i}\prod_{\ell\atop h_\ell = 1} \frac{N_\ell}{D_\ell}}{\sum_{\vect{h}\in H}\prod_{\ell\atop h_\ell = 1} \frac{N_\ell}{D_\ell}}\\
&=\frac{\sum_{\vect{h}\in H\oplus\vect{e}_i}\prod_{\ell\atop h_\ell = 1} E[y_\ell|b_\ell]}{\sum_{\vect{h}\in H}\prod_{\ell\atop h_\ell = 1} E[y_\ell|b_\ell]}
\end{align}
\end{proof}
Remark: For all $i=1,2,\ldots,n$, $E[b_i|\vect{y}]$ has the same denominator, consisting of all combinations $E_1$ through $E_n$, whose products evaluate to $1$. The numerator, on the other hand, consists of all combinations whose products evaluate to $E_i$. If we write the denominator as a polynomial of $E_i$
\begin{align*}
\mathrm{Denominator}=E_iQ_1+Q_2
\end{align*}
where posynomials $Q_1$ and $Q_2$ do not have $E_i$ in their respective expressions. The numerator, according to the closed form, is obtained as
\begin{align}
\mathrm{Numerator} &= \mathrm{Denominator} \times E_i\label{e:numerator}\\
&=E_iQ_2 + Q_1.
\end{align}
Therefore
\begin{align*}
E[b_i|\vect{y}] &= \frac{E_iQ_2+Q_1}{E_iQ_1+Q_2}\\
&=\frac{E_i+\frac{Q_1}{Q_2}}{1+E_i\frac{Q_1}{Q_2}}\\
&=\tanh\left(\tanh^{-1}(E_i)+\tanh^{-1}\left(\frac{Q_1}{Q_2}\right)\right)
\end{align*}
It is clear from (\ref{e:closedform}) that all MMSE shares the same denominator. Eq. (\ref{e:numerator}) also implies that the numerator is related to the denominator simply through multiplying $E_i$. In other words, the denominator posynomial is fundamental to MMSE decoding. For this reason, we refer to it as the characteristic posynomial of the linear block code $H$.

\section{Belief Propogation}
Belief propogation using MMSE representation has the following iterative form:

From variable node $a$ to factor node $\alpha$:
\begin{align}
m_{a\rightarrow\alpha}^{(n)}=\tanh\left(\tanh^{-1}(E_a)+\sum_{\beta\in F(a)\atop \beta \neq \alpha}\tanh^{-1}(M_{\beta\rightarrow  a}^{(n-1)})\right), \label{e:n2f}
\end{align}
and from factor node $\alpha$ to variable node $a$:
\begin{align}
M_{\alpha\rightarrow a}^{(n)} = \prod_{b\in N(\alpha)\atop b \neq a}m_{b\rightarrow \alpha}^{(n)}. \label{e:f2n}
\end{align}
with
\begin{align}
M_{\beta\rightarrow a}^{(0)} = 0.
\end{align}
After $n$ iterations ($n$ being sufficiently large), MMSE of node $a$ is recovered as
\begin{align}
E[a|\vect{y}] = \tanh\left(\tanh^{-1}(E_a)+\sum_{\beta\in F(a)}\tanh^{-1}(M_{\beta\rightarrow  a}^{(n)})\right),
\end{align}
The BP algorithm does not guarantee to converge to the correct value of $E[a|\vect{y}]$ for cyclic factor graphs. We show the proof here that convergence is guaranteed for acyclic factor graphs by induction on $p$, the number of parity checks. We use $H_p$ (resp. $H_{p-1}$) to denote the $H$ matrix from $p$ (resp. $p-1$) parity checks. If $p=1$, as shown in Fig. \ref{f:p=1}, convergence is achieved after one iteration:
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{p1}
\caption{Factor graph when $p=1$.}
\label{f:p=1}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{p}
\caption{Factor graph with $p$ factor nodes.}
\label{f:p=1}
\end{figure}
Assume it is correct up to $p-1$, now add one more factor node $\gamma$. Assume $\gamma$ connects to variable node $x, x_1, x_2, \ldots, x_g$ and $x$ is the only common variable node connected to the other $p-1$ factor nodes -- a necessay consequence if the factor graph derived from the $p$ parity checks has no cycle. Follow iterations (\ref{e:n2f}) and (\ref{e:f2n}), we have
\begin{align*}
M_{\gamma\rightarrow x}^{(1)} = E_{x_1}E_{x_2}\cdots E_{x_g}.
\end{align*}
In the iteration 2, the message from $x$ towards factor nodes other than $\gamma$ will take the form
\begin{align*}
&\tanh\left(\tanh^{-1}(E_x) + \tanh^{-1}(M_{\gamma\rightarrow x})+\cdots \right)\\
=&\tanh\left(\tanh^{-1}\left(\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}\right)+\cdots\right)
\end{align*}
This is equivalent to the graph with $p-1$ factor nodes, except all occurences of $E_x$ in the messages are to be replaced by
\begin{align*}
\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}
\end{align*}
By induction BP computes the correct MMSE for any variable node in the previous graph with $p-1$ factor nodes. Suppose it is given by
\begin{align*}
\frac{E_xQ_2+Q_1}{E_xQ_1+Q_2}
\end{align*}
where $Q_1, Q_2$ are posynomials not involving $E_x$. With the additional factor node, BP will actually compute
\begin{align*}
&\frac{\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}Q_2+Q_1}{\frac{E_x + M_{\gamma\rightarrow x}}{1+E_xM_{\gamma\rightarrow x}}Q_1+Q_2}\\
% =&\frac{E_x(Q_2+M_{\gamma\rightarrow x}Q_1)+(Q_1+M_{\gamma\rightarrow x}Q_2)}{E_x(Q_1+M_{\gamma\rightarrow x}Q_2)+(Q_2+M_{\gamma\rightarrow x}Q_1)}\\
=&\frac{E_xQ_2+Q_1+M_{\gamma\rightarrow x}Q_2 + E_xM_{\gamma\rightarrow x}Q_1}{E_xQ_1+Q_2+M_{\gamma\rightarrow x}Q_1 + E_xM_{\gamma\rightarrow x}Q_2}\\
=&\frac{E_xQ_2+Q_1+E_{x_1}E_{x_2}\cdots E_{x_g}Q_2 + E_xE_{x_1}E_{x_2}\cdots E_{x_g}Q_1}{E_xQ_1+Q_2+E_{x_1}E_{x_2}\cdots E_{x_g}Q_1 + E_xE_{x_1}E_{x_2}\cdots E_{x_g}Q_2}
\end{align*}
This is the correct MMSE with $p$ factors. Its denominator is the characteristic posynomial of $H_p$, consisting of $2^{p-1}$ monomials corresponding to $\mathrm{span}(H_{p-1})$ (i.e., $E_xQ_1+Q_2$), plus another $2^{p-1}$ monomials (i.e., $E_{x_1}E_{x_2}\cdots E_{x_g}Q_1 + E_xE_{x_1}E_{x_2}\cdots E_{x_g}Q_2$) as a consequence of adding one additional parity check $\vect{e}_x+\sum_{i=1}^g\vect{e}_i$ to $\mathrm{span}(H_{p-1})$ to form $\mathrm{span}(H_p)$. The numerator can be similarly checked or simply by noting it is $E_x$ multiplied to the denominator. For any variable node from $x_1, x_2, \ldots, x_g$, we check the MMSE of $x_1$ correctly computed, without loss of generality. $x_1$ will receive a message of the form
\begin{align*}
m_{x\rightarrow\gamma}E_{x_2}\cdots E_{x_g}
\end{align*}
where $m_{x\rightarrow\gamma}$ is the MMSE of $x$ given $p-1$ parity checks. In other words
\begin{align}
m_{x\rightarrow\gamma} = \frac{E_xQ_2+Q_1}{E_xQ_1+Q_2}
\end{align}
where 
\begin{align*}
E_xQ_1+Q_2
\end{align*}
is the characteristic posynomial of $H_{p-1}$. Node $x_1$ will then form the MMSE of itself
\begin{align}
&\tanh\left(\tanh^{-1}(E_{x_1})+\tanh^{-1}\left(m_{x\rightarrow\gamma}E_{x_2}\cdots E_{x_g}\right)\right)\\
=&\frac{E_{x_1}+\frac{E_xQ_2+Q_1}{E_xQ_1+Q_2}E_{x_2}\cdots E_{x_g}}{1+E_{x_1}E_{x_2}\cdots E_{x_g}\frac{E_xQ_2+Q_1}{E_xQ_1+Q_2}}\\
=&\frac{E_{x_1}(E_xQ_1+Q_2)+(E_xQ_2+Q_1)E_{x_2}\cdots E_{x_g}}{E_xQ_1+Q_2+E_{x_1}\cdots E_{x_g}(E_xQ_2+Q_1)}
\end{align}
One can verify the denominator is indeed the characteristic posynomial of $H_p$ and the numerator is correctly formed by multiplying $E_{x_1}$ to the denominator.

\section{Specialization}
Next we consider a few examples by specializing (\ref{e:closedform}) to different channels, computing the MMSE $E_i$ of each individual bit $i$. These MMSE values can then be subsituted into (\ref{e:closedform}) or the BP algorithm for decoding. Note both the closed form equation (\ref{e:closedform}) and the BP algorithm are the same though channels are different. This is an advantage of using MMSE decoding instead of MAP, which has to update the LLRs in each BP iteration -- the LLRs take different forms for different channels.
\subsection{BEC}
The binary erasure channel has the following conditional probability
\begin{align}
P(y_i|b_i=1) &= \begin{cases}
1-\epsilon, &y_i=1,\\
\epsilon, &y_i=0,\\
0, &y_i=-1
\end{cases}\\
P(y_i|b_i=-1) &= \begin{cases}
0, &y_i=1,\\
\epsilon, &y_i=0,\\
1-\epsilon, &y_i=-1
\end{cases}
\end{align}
where $\epsilon$ is the erasure probability (0 at the output represents erasure). Therefore
\begin{align}
P(y_i|b_i=1)-P(y_i|b_i=-1) &= (1-\epsilon)y_i, \\
P(y_i|b_i=1)+P(y_i|b_i=-1) &= \begin{cases}
1-\epsilon, &y_i=\pm 1,\\
2\epsilon, &y_i = 0.
\end{cases}
\end{align}
Hence by (\ref{e:single_observation_mmse}),
\begin{align}
E_i = y_i.
\end{align}
\subsection{BSC}
The memoryless binary symmetric channel has the following conditional probability
\begin{align}
P(y_i|b_i) = \frac{(1-2\epsilon)b_iy_i+1}{2},
\end{align}
where $\epsilon$ is the crossover probability and $b_i, y_i$ take on vlues from $\{-1, 1\}$. Plug it into (\ref{e:single_observation_mmse}) we have
\begin{align}
E_i = (1-2\epsilon)y_i
\end{align}
\subsection{AWGN}
Assume channel is additive white Gaussian with variance $\sigma^2$, i.e.,
\begin{align}
Y = X + Z
\end{align}
where $Z\sim \mathcal{N}(0, \sigma^2)$, we have
\begin{align}
P(y_i|b_i) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-b_i)^2}{2\sigma^2}}.
\end{align}
Therefore
\begin{align}
E_i = \tanh\left(\frac{y_i}{\sigma^2}\right)=\tanh(\mathrm{SNR}\cdot y_i).
\end{align}
\end{document}

